{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weight Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have a Tensor (B,T,C) where B is Batch, T is time (no of words in context), and C is (channel) embedding for each token\n",
    "#We want to have one Token talk to other token but only previous to it. (Like token 2 can talk to 0,1 and not to 3 and so forth)\n",
    "B = 2\n",
    "T = 5\n",
    "C = 3\n",
    "xb = torch.rand((B,T,C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2357, 0.6186, 0.3991],\n",
       "         [0.1029, 0.5228, 0.8372],\n",
       "         [0.9153, 0.9635, 0.1719],\n",
       "         [0.0969, 0.2464, 0.5394],\n",
       "         [0.3521, 0.2202, 0.7020]],\n",
       "\n",
       "        [[0.7796, 0.2707, 0.5217],\n",
       "         [0.1820, 0.5898, 0.4388],\n",
       "         [0.4741, 0.3602, 0.0515],\n",
       "         [0.1895, 0.8833, 0.1996],\n",
       "         [0.2332, 0.0990, 0.9172]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb\n",
    "#Shape Type\n",
    "# [[                        #Channel1\n",
    "#   [Word1 Embedding]\n",
    "#   [Word2 Embedding]\n",
    "#   [Word3 Embedding]\n",
    "#  ]\n",
    "#  [                        #Channel2\n",
    "#   [Word1 Embedding]\n",
    "#   [Word2 Embedding]\n",
    "#   [Word3 Embedding]\n",
    "#  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):              #for each batch \n",
    "    for t in range(T):          #for each word\n",
    "        xprev = xb[b,:t+1]  #(t,C)\n",
    "        xbow[b,t] = torch.mean(xprev,0)  #around dim = 0 meaning xbow[b,t] = (C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2357, 0.6186, 0.3991],\n",
       "         [0.1693, 0.5707, 0.6182],\n",
       "         [0.4180, 0.7016, 0.4694],\n",
       "         [0.3377, 0.5878, 0.4869],\n",
       "         [0.3406, 0.5143, 0.5299]],\n",
       "\n",
       "        [[0.7796, 0.2707, 0.5217],\n",
       "         [0.4808, 0.4303, 0.4803],\n",
       "         [0.4786, 0.4069, 0.3373],\n",
       "         [0.4063, 0.5260, 0.3029],\n",
       "         [0.3717, 0.4406, 0.4258]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow\n",
    "#Shape Type\n",
    "# [ [ Word1 New Embedding]\n",
    "#   [ Word2 New Embedding = Word1 + Word2 / 2]\n",
    "#   [ Word3 New Embedding = Word1 + Word2 + Word3 / 3] \n",
    "#...\n",
    "#  ]\n",
    "\n",
    "# By using mean as calculation we are creating new embedidng where each word embedding is dependent on its values and previous words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demo Matrix Multiplication for Communication between words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are now exploring the same thing with multiplication as calculative step.\n",
    "\n",
    "# a is just a helping matrix\n",
    "# b is xb                       (input)\n",
    "# c is xbow                     (output)    \n",
    "a = torch.ones(3,3)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9., 9.],\n",
       "        [9., 5.],\n",
       "        [2., 5.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[20., 19.],\n",
       "        [20., 19.],\n",
       "        [20., 19.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem:\n",
    "#   1. Our current is effected by future words.\n",
    "#Solutin:\n",
    "#   1. Masking for preventing future token access (chaning the a matrix to lowe triangular matrix)\n",
    "a = torch.tril(torch.ones(3,3))\n",
    "c = a @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9., 9.],\n",
       "        [9., 5.],\n",
       "        [2., 5.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9.,  9.],\n",
       "        [18., 14.],\n",
       "        [20., 19.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Still we are summing the values of all the tokens. We need to change it to mean.\n",
    "#Change a matrix again;\n",
    "a = torch.tril(torch.ones(3,3))\n",
    "a = a / torch.sum(a,1,keepdim=True)\n",
    "c = a @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9., 9.],\n",
       "        [9., 5.],\n",
       "        [2., 5.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.0000, 9.0000],\n",
       "        [9.0000, 7.0000],\n",
       "        [6.6667, 6.3333]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can achieve the same thing with a single matrix multiplication (without using for loop) by using broadcasting.\n",
    "\n",
    "wei = torch.tril(torch.ones(T,T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ xb    #(T,T) -> (B,T,T) Batch Multiplication    # (B,T,T) @ (B,T,C) -> (B,T,C)\n",
    "\n",
    "#torch.allclose check is two tensor are matching or not (element wise) (works with floating type as it check with some tolerance)\n",
    "torch.allclose(xbow,xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5008, 0.8865, 0.6845],\n",
       "         [0.1078, 0.5549, 0.4455],\n",
       "         [0.5259, 0.5692, 0.6997],\n",
       "         [0.5621, 0.8024, 0.0466],\n",
       "         [0.7508, 0.4546, 0.5460]],\n",
       "\n",
       "        [[0.7871, 0.9321, 0.0516],\n",
       "         [0.1905, 0.2732, 0.1106],\n",
       "         [0.4148, 0.5582, 0.4033],\n",
       "         [0.4960, 0.7807, 0.5988],\n",
       "         [0.8282, 0.4706, 0.7160]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5008, 0.8865, 0.6845],\n",
       "         [0.3043, 0.7207, 0.5650],\n",
       "         [0.3782, 0.6702, 0.6099],\n",
       "         [0.4242, 0.7033, 0.4691],\n",
       "         [0.4895, 0.6535, 0.4845]],\n",
       "\n",
       "        [[0.7871, 0.9321, 0.0516],\n",
       "         [0.4888, 0.6026, 0.0811],\n",
       "         [0.4642, 0.5878, 0.1885],\n",
       "         [0.4721, 0.6361, 0.2911],\n",
       "         [0.5433, 0.6030, 0.3761]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones((T,T)))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei,dim=-1)\n",
    "xbow3 = wei @ xb\n",
    "torch.allclose(xbow,xbow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((T,T)).masked_fill(tril == 0, float('-inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(torch.zeros((T,T)).masked_fill(tril == 0, float('-inf')),dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single Head of Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 3])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Input\n",
    "B = 2\n",
    "T = 5 \n",
    "C = 3\n",
    "xb = torch.rand((B,T,C))\n",
    "xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_size = 16\n",
    "#Every token at each position will give one key vector and one query vector and one value vector\n",
    "#Query (what are u looking for)\n",
    "#Key (what do i contain)\n",
    "\n",
    "#my query dot product with all other key to get wei matrix\n",
    "key = nn.Linear(C,head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = key(xb)     #(B,T,16)\n",
    "q = query(xb)   #(B,T,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5, 3]), torch.Size([2, 5, 16]), torch.Size([2, 5, 16]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape, k.shape, q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 5])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = q @ k.transpose(-2,-1)        #Transpose dim = -1 with -2 (T,16) -> (16,T)\n",
    "wei.shape       #(B,T,T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 5])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.transpose(-2,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = wei.masked_fill(tril== 0, float('-inf'))\n",
    "wei = F.softmax(wei,dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5280, 0.4720, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3104, 0.3080, 0.3816, 0.0000, 0.0000],\n",
       "        [0.2757, 0.2216, 0.2749, 0.2279, 0.0000],\n",
       "        [0.1672, 0.2185, 0.2319, 0.2172, 0.1652]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 3])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Output\n",
    "out = wei @ xb\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0516, 0.7512, 0.2985],\n",
       "         [0.3308, 0.4479, 0.2216],\n",
       "         [0.4993, 0.4662, 0.1734],\n",
       "         [0.5183, 0.5189, 0.2925],\n",
       "         [0.5848, 0.5365, 0.4767]],\n",
       "\n",
       "        [[0.3966, 0.5235, 0.4951],\n",
       "         [0.2870, 0.4561, 0.6321],\n",
       "         [0.2080, 0.3668, 0.6084],\n",
       "         [0.2775, 0.3504, 0.5227],\n",
       "         [0.3824, 0.3633, 0.4628]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 16])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = nn.Linear(C, head_size, bias = False)\n",
    "v = value(xb)\n",
    "out = wei @ v\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonGenAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
