{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov  2 12:14:24 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 556.13                 Driver Version: 556.13         CUDA Version: 12.5     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   55C    P0             19W /   95W |       0MiB /   6144MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "Attributes\n",
    "- torch.dtype\n",
    "    - torch.FloatTensor, ...\n",
    "    - torch.cuda.FloatTensor, ...\n",
    "- torch.device\n",
    "- torch.layout\n",
    "- torch.Tensor.grad\n",
    "    - None by default\n",
    "    - becomes tensor when first time called backward()\n",
    "    - gradient are accumulated on future calls\n",
    "- torch.Tensor.requires_grad\n",
    "\n",
    "Functions  \n",
    "- torch.Tensor.backward()\n",
    "    - computes gradient of current tensor wrt graph leaves\n",
    "    - triggers all tensor gradient in prev layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Always copies the data\n",
    "#torch.tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7)\n",
      "0\n",
      "7\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "#Scalars\n",
    "    #ndim = 0\n",
    "scalar = torch.tensor(7)\n",
    "print(scalar)\n",
    "print(scalar.ndim)\n",
    "print(scalar.item())\n",
    "print(scalar.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 7])\n",
      "1\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "#Vector\n",
    "    #ndim = 1\n",
    "    #shape = array size\n",
    "vector = torch.tensor((7,7))\n",
    "print(vector)\n",
    "print(vector.ndim)\n",
    "print(vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7,  8],\n",
      "        [ 9, 10]])\n",
      "2\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "#matrix\n",
    "    #ndim = 2\n",
    "    #shape = row, col\n",
    "MATRIX = torch.tensor( [[7,8],[9,10]])\n",
    "print(MATRIX)\n",
    "print(MATRIX.ndim)\n",
    "print(MATRIX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12]]])\n",
      "3\n",
      "torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "#tensor\n",
    "    #ndim > 2\n",
    "    #shape\n",
    "TENSOR = torch.tensor( [[ [1,2,3],[4,5,6] ], [ [7,8,9],[10,11,12]] ])\n",
    "print(TENSOR)\n",
    "print(TENSOR.ndim)\n",
    "print(TENSOR.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BuiltIn Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5255, 0.6119, 0.7445, 0.1212],\n",
      "        [0.1954, 0.6378, 0.3828, 0.1275],\n",
      "        [0.3558, 0.8873, 0.5513, 0.6215]])\n",
      "tensor([[0.5240, 0.4560, 0.0069, 0.7626],\n",
      "        [0.5871, 0.1299, 0.9513, 0.4457],\n",
      "        [0.6905, 0.0871, 0.4863, 0.3944]])\n"
     ]
    }
   ],
   "source": [
    "#Random Tensor\n",
    "print(torch.rand(3,4))\n",
    "print(torch.rand(size=(3,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "#01 Tensor\n",
    "print(torch.zeros(2,3))\n",
    "print(torch.ones(2,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "tensor([0, 2, 4, 6, 8])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(torch.arange(0,10))\n",
    "print(torch.arange(start=0,end=10,step=2))\n",
    "print(torch.zeros_like(input=torch.arange(0,10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 5, 6])\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "tensorInt = torch.tensor([3,5,6],\n",
    "                   dtype=None,      # float32/float float64/double  float16/half (default)\n",
    "                   device=\"cpu\",     # \"cpu\"  \"cuda\"  None\n",
    "                   requires_grad=False) #\n",
    "print(tensorInt)\n",
    "print(tensorInt.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. Add\n",
    "    2. Sub  \n",
    "    3. Multiply \n",
    "    4. Div\n",
    "    5. Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([11, 12, 13])\n",
      "tensor([11, 12, 13])\n"
     ]
    }
   ],
   "source": [
    "#Scalar Add\n",
    "tensor = torch.tensor([1,2,3])\n",
    "print(tensor + 10)\n",
    "print(torch.add(tensor,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10, 20, 30])\n",
      "tensor([10, 20, 30])\n"
     ]
    }
   ],
   "source": [
    "#Scalar Multiplication\n",
    "print(tensor * 10)\n",
    "print(torch.mul(tensor,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1000, 0.2000, 0.3000])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scalar Divide\n",
    "tensor / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 4, 9])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hadamard Product\n",
    "tensor * tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dot Product or matrix Multiplication\n",
    "torch.matmul(tensor,tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "torch.matmul(torch.rand(100,100,100),torch.rand(100,100,100))\n",
    "print(\"Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1526], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "w = torch.rand(1, requires_grad=True)\n",
    "x = torch.rand(1)\n",
    "b = torch.rand(1, requires_grad=True)\n",
    "y = w * x + b\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6333, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "w = torch.rand(size=(3,4), requires_grad=True)\n",
    "x = torch.rand(size=(4,5), requires_grad=True)\n",
    "b = torch.rand(size=(3,5), requires_grad=True)\n",
    "y = torch.matmul(w,x) + b\n",
    "y.retain_grad()\n",
    "avg = y.mean()\n",
    "print(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1507, 0.1507, 0.1507, 0.1507, 0.1507],\n",
      "        [0.1120, 0.1120, 0.1120, 0.1120, 0.1120],\n",
      "        [0.0418, 0.0418, 0.0418, 0.0418, 0.0418],\n",
      "        [0.1122, 0.1122, 0.1122, 0.1122, 0.1122]])\n",
      "tensor([[0.1762, 0.1898, 0.1430, 0.1288],\n",
      "        [0.1762, 0.1898, 0.1430, 0.1288],\n",
      "        [0.1762, 0.1898, 0.1430, 0.1288]])\n",
      "tensor([[0.0667, 0.0667, 0.0667, 0.0667, 0.0667],\n",
      "        [0.0667, 0.0667, 0.0667, 0.0667, 0.0667],\n",
      "        [0.0667, 0.0667, 0.0667, 0.0667, 0.0667]])\n",
      "tensor([[0.0667, 0.0667, 0.0667, 0.0667, 0.0667],\n",
      "        [0.0667, 0.0667, 0.0667, 0.0667, 0.0667],\n",
      "        [0.0667, 0.0667, 0.0667, 0.0667, 0.0667]])\n"
     ]
    }
   ],
   "source": [
    "avg.backward()\n",
    "print(x.grad)   #davg/dx\n",
    "print(w.grad)   #davg/dw\n",
    "print(b.grad)   #davg/db\n",
    "print(y.grad)   #davg/dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7261)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m     avg \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(avg)\n\u001b[1;32m---> 14\u001b[0m \u001b[43mavg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mgrad)   \u001b[38;5;66;03m#davg/dx\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(w\u001b[38;5;241m.\u001b[39mgrad)   \u001b[38;5;66;03m#davg/dw\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\revai\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\revai\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\revai\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "#Required_grad\n",
    "with torch.no_grad():\n",
    "    #any computation done here will not have loss.backward()/ tensor.backward()\n",
    "    #apply when u are sure not to call tensor.backward()\n",
    "    #even if input is requires_grad(True) --> no grad calculation\n",
    "    #memory saved \n",
    "    w = torch.rand(size=(3,4), requires_grad=True)\n",
    "    x = torch.rand(size=(4,5), requires_grad=True)\n",
    "    b = torch.rand(size=(3,5), requires_grad=True)\n",
    "    y = torch.matmul(w,x) + b\n",
    "    avg = y.mean()\n",
    "    print(avg)\n",
    "\n",
    "avg.backward()\n",
    "print(x.grad)   #davg/dx\n",
    "print(w.grad)   #davg/dw\n",
    "print(b.grad)   #davg/db\n",
    "\n",
    "\n",
    "#As decorator\n",
    "@torch.no_grad()\n",
    "def inference_function():\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Views\n",
    "Allows different shaped tensor without creating a copy\n",
    "\n",
    "Exceptions:\n",
    "- reshape()\n",
    "- reshape_as()\n",
    "- flatten()\n",
    "- contiguous()  \n",
    "\n",
    "These may/may not return a view and therefore may create a new tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
